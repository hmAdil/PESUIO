{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)\n",
    "\n",
    "# Data Normalization and Standardization\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)\n",
    "x_train = (x_train - mean) / (std + 1e-7)\n",
    "x_test = (x_test - mean) / (std + 1e-7)\n",
    "\n",
    "# Reshape to maintain the 2D structure with a channel dimension (for grayscale images)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# One-Hot Encoding\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_encode = one_hot_encode(y_train, 10)\n",
    "y_test_encode = one_hot_encode(y_test, 10)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,        # Randomly rotate images by up to 10 degrees\n",
    "    width_shift_range=0.1,    # Randomly shift images horizontally by 10%\n",
    "    height_shift_range=0.1,   # Randomly shift images vertically by 10%\n",
    "    zoom_range=0.1,           # Randomly zoom into images by up to 10%\n",
    "    horizontal_flip=False     # Avoid flipping as fashion items are not symmetric\n",
    ")\n",
    "\n",
    "# Fit the generator to the training data\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Neural Network Model\n",
    "class NN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, l1_lambda=0.0, l2_lambda=0.0):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01 \n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.loss_history = []\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / (np.sum(exp_x, axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "    def forward(self, X): \n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = np.tanh(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        m = X.shape[0]\n",
    "        output_error = output - y\n",
    "        hidden_error = np.dot(output_error, self.W2.T) * (1 - np.tanh(self.z1) ** 2)\n",
    "\n",
    "        dW2 = np.dot(self.a1.T, output_error) / m\n",
    "        db2 = np.sum(output_error, axis=0, keepdims=True) / m\n",
    "        dW1 = np.dot(X.T, hidden_error) / m\n",
    "        db1 = np.sum(hidden_error, axis=0, keepdims=True) / m\n",
    "\n",
    "        dW1 += self.l1_lambda * np.sign(self.W1)\n",
    "        dW2 += self.l1_lambda * np.sign(self.W2)\n",
    "\n",
    "        dW1 += self.l2_lambda * self.W1\n",
    "        dW2 += self.l2_lambda * self.W2\n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size=128, regularization_type=\"L2\"):\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in datagen.flow(X, y, batch_size=batch_size):\n",
    "                output = self.forward(X_batch.reshape(X_batch.shape[0], -1))\n",
    "                self.backward(X_batch.reshape(X_batch.shape[0], -1), y_batch, output)\n",
    "\n",
    "            if regularization_type == \"L1\":\n",
    "                penalty = self.l1_lambda * (np.sum(np.abs(self.W1)) + np.sum(np.abs(self.W2)))\n",
    "            else:\n",
    "                penalty = self.l2_lambda * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))\n",
    "\n",
    "            loss = -np.mean(np.sum(y * np.log(output + 1e-10), axis=1)) + penalty\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X.reshape(X.shape[0], -1))\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "# Initialize models with L1 and L2 regularization\n",
    "nn_l1 = NN(input_size=784, hidden_size=64, output_size=10, learning_rate=0.001, l1_lambda=0.0001)\n",
    "nn_l2 = NN(input_size=784, hidden_size=64, output_size=10, learning_rate=0.001, l2_lambda=0.001)\n",
    "\n",
    "epochs = 25\n",
    "nn_l1.train(x_train, y_train_encode, epochs=epochs, regularization_type=\"L1\")\n",
    "nn_l2.train(x_train, y_train_encode, epochs=epochs, regularization_type=\"L2\")\n",
    "\n",
    "# Plot loss history\n",
    "plt.plot(nn_l1.loss_history, label='L1 Regularization')\n",
    "plt.plot(nn_l2.loss_history, label='L2 Regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('L1 vs L2 Regularization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and print test accuracy\n",
    "predictions_l1 = nn_l1.predict(x_test)\n",
    "accuracy_l1 = np.mean(predictions_l1 == y_test)\n",
    "print(f\"Test Accuracy of L1: {accuracy_l1:.4f}\")\n",
    "\n",
    "predictions_l2 = nn_l2.predict(x_test)\n",
    "accuracy_l2 = np.mean(predictions_l2 == y_test)\n",
    "print(f\"Test Accuracy of L2: {accuracy_l2:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "def plot_predictions(X, y_true, y_pred, num_images=10):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f'True: {y_true[i]}\\nPred: {y_pred[i]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(x_test, y_test, predictions_l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
