{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6078e50-8791-4818-891a-edaa3045a44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.datasets import mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bee2ac1-7383-4cc9-9fcd-007c67373c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c114fbe-a575-42a7-97ab-9f83a10e1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355a3bd-99c2-41cf-b75d-75d3628b0a52",
   "metadata": {},
   "source": [
    "## Load the MNIST Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712741fc-359a-47b9-878d-84292c52c469",
   "metadata": {},
   "source": [
    "`Flattening`: The reshape(x_train.shape[0], -1) method transforms the training data (x_train) from its original multi-dimensional shape (e.g., for images, this could be width x height x channels) into a two-dimensional array. Each sample is converted into a single row, where the first dimension is the number of samples, and the second dimension contains all the pixel values in a flattened format. This is essential for feeding the data into models that expect a flat input, such as fully connected layers in neural networks.\n",
    "\n",
    "`Normalization`: The division by 255.0 scales the pixel values to a range between 0 and 1, as pixel values in images typically range from 0 to 255. Normalization helps improve model training by ensuring that the input features have similar scales, which enhances convergence speed and overall model performance.\n",
    "\n",
    "`Combined Effect`: By flattening and normalizing the data simultaneously, the code prepares the input data for effective processing by machine learning models, ensuring that they can learn efficiently from the data while reducing the risk of issues caused by varying input scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af3d578c-66f5-4be6-a3b7-b5d024a882e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e6c2485-b7d0-451f-94a8-09c421990157",
   "metadata": {},
   "source": [
    "## Time to one-hot encode the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4a769a6-84ae-4311-8022-2f4086b2444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3d17d-bfa2-440c-b802-071b8c9eca74",
   "metadata": {},
   "source": [
    "One-hot encoding is applied only to the target variable (y_train and y_test) to convert categorical class labels into a binary format suitable for classification tasks. This transformation allows the model to output probabilities for each class and avoids implying any ordinal relationships. The input features (x_train and x_test) are typically left in their original format to preserve their numerical or categorical significance for effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52680bb7-54f1-4437-9014-ad69814c0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = one_hot_encode(y_train, 10) \n",
    "y_test_encoded = one_hot_encode(y_test, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a899d-f109-4809-83aa-b6a9cef5b1f9",
   "metadata": {},
   "source": [
    "# Let's define a Neural Network Now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89de8137-141b-415a-80eb-09d74aa8fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import NumPy for numerical operations\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights and biases for input to hidden layer\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01  # Small random weights\n",
    "        self.b1 = np.zeros((1, hidden_size))  # Biases initialized to zero\n",
    "        \n",
    "        # Initialize weights and biases for hidden to output layer\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01  # Small random weights\n",
    "        self.b2 = np.zeros((1, output_size))  # Biases initialized to zero\n",
    "        \n",
    "        self.learning_rate = learning_rate  # Set learning rate\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Compute the softmax of the input for multi-class classification\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # Normalize to get probabilities\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Perform forward propagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # Linear combination for hidden layer\n",
    "        self.a1 = np.tanh(self.z1)  # Activation function for hidden layer (tanh)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Linear combination for output layer\n",
    "        self.a2 = self.softmax(self.z2)  # Softmax activation for output layer\n",
    "        return self.a2  # Return the output probabilities\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Perform backward propagation to update weights and biases\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Calculate the error at the output layer\n",
    "        output_error = output - y  # Derivative of loss w.r.t. output\n",
    "        \n",
    "        # Calculate the error at the hidden layer\n",
    "        hidden_error = np.dot(output_error, self.W2.T) * (1 - np.tanh(self.z1) ** 2)  # Derivative of tanh\n",
    "        \n",
    "        # Compute gradients\n",
    "        dW2 = np.dot(self.a1.T, output_error) / m  # Gradient for W2\n",
    "        db2 = np.sum(output_error, axis=0, keepdims=True) / m  # Gradient for b2\n",
    "        dW1 = np.dot(X.T, hidden_error) / m  # Gradient for W1\n",
    "        db1 = np.sum(hidden_error, axis=0, keepdims=True) / m  # Gradient for b1\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        self.W1 -= self.learning_rate * dW1  # Update W1\n",
    "        self.b1 -= self.learning_rate * db1  # Update b1\n",
    "        self.W2 -= self.learning_rate * dW2  # Update W2\n",
    "        self.b2 -= self.learning_rate * db2  # Update b2\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        # Train the neural network for a specified number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)  # Forward pass\n",
    "            self.backward(X, y, output)  # Backward pass\n",
    "            \n",
    "            # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                loss = -np.mean(np.sum(y * np.log(output + 1e-10), axis=1))  # Cross-entropy loss\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions for input data\n",
    "        output = self.forward(X)  # Forward pass to get output\n",
    "        return np.argmax(output, axis=1)  # Return the class with the highest probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55b1aa5-d8ca-41d5-9714-858a37683f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784  # 28x28 pixels\n",
    "hidden_size = 128  # Number of neurons in the hidden layer\n",
    "output_size = 10   # 10 classes for digits 0-9\n",
    "learning_rate = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149f221-b151-47ec-ae51-42af6012e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "nn.train(x_train, y_train_encoded, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d6278-aa88-4c27-84ad-cae423c1e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = nn.predict(x_test)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc559ff4-f301-4e68-aeb9-743492193342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def plot_predictions(X, y_true, y_pred, num_images=10):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f'True: {y_true[i]}\\nPred: {y_pred[i]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions on test set\n",
    "plot_predictions(x_test, y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PESIO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
